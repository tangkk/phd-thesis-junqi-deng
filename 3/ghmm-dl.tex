% this file is called up by thesis.tex
% content in this file will be fed into the main document

%: ----------------------- name of chapter  -------------------------
\chapter{A Hybrid GMM-HMM and Deep Neural Nets Approach}\label{cp:ghmm}

%As recapped in Section~\ref{sec:2-rnnpm}, by far there are four types of ACE approaches:
%\begin{itemize}
%\item local feature extraction - global smoothing \cite{fujishima1999realtime,sheh2003chord}
%\item local classification - global smoothing \cite{humphrey2012rethinking};
%\item local feature learning - global classification - global smoothing \cite{boulanger2013audio,sigtia2015audio};
%\item local feature learning - local classification - global smoothing \cite{zhou2015chord}.
%\end{itemize}
%Note that the global smoothing process is actually equivalent to a domain knowledge driven global segmentation process and a global classification process running at the same time.

%All these approaches apply similar methodologies in ASR \cite{deng2014deep,bourlard2012connectionist} directly to ACE, but they overlook a fundamental difference, that chords are usually segmented rhythmically and chords sustains much longer than phonemes. This crucial divergence could lead to some different approaches.

Thus far, we have identified several research gaps. Firstly, the existing works have not considered a fundamental difference between ASR and ACE in regards to segmentation, which may lead to a possible design that considers segmentation and classification as two separate tasks. Secondly, the support for large vocabulary has been largely overlooked, particularly the support for chord inversions. Note that chord inversion is a crucial ingredient for pop and rock music, which is the primary focus of ACE research.

This chapter introduces an LVACE system framework that leverages the rhythmic property of chords. It can be categorized as a \textbf{``local feature extraction - global segmentation - local classification''} approach, which isolates the process of segmentation from classification. Specifically, the classification process is implemented via deep neural nets. We would like to verify whether this approach could benefit the LVACE performance, in particular the performance on the \textit{SeventhsBass} vocabulary.

Experimental results show that among the three proposed deep neural nets and a baseline model, the RNN based system has the best average chord quality accuracy that significantly outperforms the other considered models. Furthermore, our bias-variance analysis has identified a glass ceiling as a potential hindrance to future improvements of large vocabulary automatic chord estimation systems.

%: ----------------------- contents from here ------------------------

\section{System Framework} \label{sec:3-sysframe}

Figure~\ref{fig:3-sysover} depicts the LVACE system framework in our study. The workflow is as follows:
\begin{itemize}
	\item Feature extraction: both training and validation data share the same feature extraction module; features are extracted from each input track using a method similar to the one employed in the Chordino system~\cite{mauch2010automatic} (to be elaborated in Section~\ref{sec:3-fe}).
	\item Segmentation: 1. for training, the feature sequence is segmented by the ground truth annotations; 2. for validation, the feature sequence is segmented using a GMM-HMM process (to be discussed in Section~\ref{sec:3-sg}).
	\item Segment tiling: each feature segment is tiled into a fixed number of sub-segments (see Section~\ref{sec:3-seg-tile}).
	\item Deep neural nets: 1. for training, the segments and their chord labels are used to train the deep neural nets (will be described in Section~\ref{sec:3-dlmodel}); 2. for validation, the trained neural network is used to predict chord labels.
\end{itemize}

\begin{figure}
\centering
\includegraphics[width=0.5\columnwidth]{3/figures/sys.pdf}
\caption{GMM-HMM ACE System framework.}
\label{fig:3-sysover}
\end{figure}

\subsection{Feature Extraction} \label{sec:3-fe}
Feature extraction starts by resampling the raw audio input at 11025 Hz, which is followed by an STFT (with 4096-point Hamming window, 512-point hop size). It then proceeds to transform the linear-frequency spectrogram (2049-bin) to log-frequency spectrogram (252-bin, three bins per semitone ranging from MIDI note 21 to 104) using two cosine interpolation kernels \cite{mauch2010automatic}. The output at this step is a log-frequency-spectrogram, or log-spectrogram, $Y_{k,m}$, where $k$ is the index of frequency bins, and $m$ is the index of time frames. We denote the total number of frames as $M$, and the total number of bins in each spectrum as $K$ (in this context $K$ = 252).

The amount of deviation from standard tuning is estimated using the algorithm in \cite{dressler2007tuning}, where the amount of detuning is estimated as:
\begin{equation}\label{eq:3-tuning}
	\delta = { {wrap(-\varphi-{2\pi}/3)} \over {2\pi} },
\end{equation}
where $wrap$ is a function wrapping its input to $[-\pi,\pi)$. $\varphi$ is the phase angle at $2\pi/3$ of the discrete-Fourier-transform (DFT) of ${\sum_m Y_{k,m}} / M$. The tuning frequency $\tau$ is then computed as:
\begin{equation}
\tau=440\cdot2^{\delta/12},
\end{equation}
and the original tuning is updated by interpolating the original spectrogram $Y_{k,\cdot}$ at $Y_{{k+p},\cdot}$, where:
\begin{equation}
p = (\log(\tau / 440) / \log(2)) \times 36.
\end{equation}
The ``36'' indicates that there are 36 bins per octave (3 bins per semitone) in $Y_{k,\cdot}$. The updated $Y_{k,m}$ spectrogram will be referred to as ``notegram'' in the following.

To enhance harmonic content and attenuate background noise, a standardization process is performed along the frequency axis:
\begin{equation}
Y^{STD}_{k,\cdot} = 
\begin{cases}
{{Y_{k,\cdot} - \mu_{k,\cdot}} \over \sigma_{k,\cdot}}, \text{\quad\quad\quad if } Y_{k,\cdot} > \mu_{k,\cdot}\\
0 \quad\quad\quad\quad\quad\quad\quad\text{otherwise,}
\end{cases}
\end{equation}
where $\mu_{k,\cdot}$ and $\sigma_{k,\cdot}$ are the mean and standard deviation of a half-octave window centered at $Y_{k,\cdot}$, respectively. This is followed by an NNLS method  (equation~\ref{eq:2-nnls}) to extract note activation patterns (84-bin, 1 bin per semitone) \cite{mauch2010approximate}.

The feature dimension is further reduced before the segmentation process. Particularly, each NNLS chroma is weighted by the bass and treble profiles depicted in Figure~\ref{fig:3-btprofile}. After that the saliences of each pitch class are added together, resulting in a 24-bin bass-treble chromagram. Each column of the bass-treble chromagram is then $L_\infty$ normalized, so that each bin of each chroma is within the range of [0,1].

We mainly make use of two features from above process: the notegram and the bass-treble chromagram (refered to as ``chromagram'' in the following).

\begin{figure}[htb]
\centering
\includegraphics[width=0.6\columnwidth]{3/figures/btp.pdf}
\caption{Bass (+) and treble (.) profiles. They are both computed in the shape of Rayleigh distributions with scale parameters 16.8 (for bass) and 42 (for treble) respectively. The horizontal axis stands for MIDI pitch numbers. The vertical axis represents normalized profile amplitudes from 0 to 1.}
\label{fig:3-btprofile}
\end{figure}

\begin{table}[htb]
\caption{Different levels of feature representations}
\centering
\footnotesize
\begin{tabular}{|c|c|c|} \hline
 Process & Output level & Bins \\ \hline
 STFT & spectrogram & 2049 \\ \hline
 Tuning & notegram (-ns) & 252 \\ \hline
 Bass-treble Profiling & chromagram (-ch) & 24 \\ \hline
\end{tabular}
\label{tab:3-felevels}
\end{table}

Here are some simple showcases of the notegrams, all of which are extracted from acoustic piano audio: Figure~\ref{fig:3-note_C} illustrates the notegram of a middle C note, Figure~\ref{fig:3-interval_CE} a major third interval built upon the middle C, and Figure~\ref{fig:3-chord_Cmaj} a C major chord rooted on the same note. Notice the appearance of harmonics and background noise.

\begin{figure}
\centering
\includegraphics[width=0.6\columnwidth]{3/figures/note_C.pdf}
\caption{Middle C note - notegram}
\label{fig:3-note_C}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.6\columnwidth]{3/figures/interval_CE.pdf}
\caption{Major third interval - notegram}
\label{fig:3-interval_CE}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.6\columnwidth]{3/figures/chord_Cmaj.pdf}
\caption{C major chord - notegram}
\label{fig:3-chord_Cmaj}
\end{figure}

The above are all simple music elements with pure pitches. Figure~\ref{fig:3-cp} is a notegram example of the intro (first few seconds) of {\it Let it be}. As this is just a piano intro, no vocal melody is involved. Notice how it is rhythmically segmented.
\begin{figure}
\centering
\includegraphics[width=0.5\columnwidth]{3/figures/cp.png}
\caption{Chord progression example - notegram}
\label{fig:3-cp}
\end{figure}

Figure~\ref{fig:3-cpm} showcases another example with vocal melody. It is extracted from the first line of the first verse of {\it Let it be}. Notice that although this is much more corrupted, a rhythmical segmentation is also visually noticeable.
\begin{figure}
\centering
\includegraphics[width=0.5\columnwidth]{3/figures/cpm.png}
\caption{Chord progression with vocal melody example - notegram}
\label{fig:3-cpm}
\end{figure}

\newpage
\subsection{Segmentation} \label{sec:3-sg}
The segmentation process is implemented using a GMM-HMM, which is characterized as follows:
\begin{itemize}
	\item The hidden node models the categorical states of chords. In the \textit{SeventhsBass} vocabulary, there are totally 217 states (1 state per chord), where the 217 is found by multiplying the number of chord types (18) with the number of chord roots (12) and adding the number of ``N.C.'' chord types (1).
	
	\item The observable node represents a chroma. It is a 24-dimension Gaussian node connecting to the bass-treble chromagram.
	
	\item The emission of each hidden state is a 24-dimension Gaussian distribution with parameters specified in Table~\ref{tab:3-gaussian}. These parameters assign different Gaussians to different pitch classes according to their roles in a chord.
	
	\item The transition matrix has heavy uniform self-transition weights, which are 99.99 times of the uniform non-self-transition weight.
	
	\item The prior probabilities are uniformly distributed.
\end{itemize}

\begin{table}
\caption{GMM-HMM segmentation settings. Different parameters are assigned to different note degree within a chord.}
\centering
\footnotesize
\begin{tabular}{|c|c|c|} \hline
      & $\mu$ & $\sigma^2$ \\ \hline
 Bass - chord bass & 1 & 0.1 \\ \hline
 Bass - not chord bass but chord note & 1 & 0.5  \\ \hline
 Bass - not bass & 0 & 0.1 \\ \hline
 Treble - chord note & 1 & 0.2  \\ \hline
 Treble - not chord note & 0 & 0.2 \\ \hline
 N.C. (for all notes)  & 1 & 0.2  \\ \hline
\end{tabular}
\label{tab:3-gaussian}
\end{table}

%\begin{figure}[htb]
%\centering
%\includegraphics[width=0.4\columnwidth]{3/figures/hmm1.pdf}
%\caption{GMM-HMM Segmentation.}
%\label{fig:3-hmm1}
%\end{figure}

Figure~\ref{fig:3-cdflow} summarizes the full information flow of the above feature extraction and segmentation process using the first line of {\it Let it be} as input. Note how representations are enhanced or compressed during this process.
\begin{figure}[h]
\centering
\includegraphics[width=1\columnwidth,angle =90]{3/figures/cdflow.pdf}
\caption{Information flow of feature extraction and segmentation}
\label{fig:3-cdflow}
\end{figure}

\subsection{Segment Tiling Process} \label{sec:3-seg-tile}

The segment tiling process is introduced to equalize the length of every segment, so as to enable neural networks with fixed-length input. This process divides a segment into $N$ equal-sized sub-segments, and takes a frame-average within each sub-segment, resulting in an $N$-frame segment (referred to as $N$seg in the following, where $N$ is a variable). If the original number of frames is not divisible by $N$, the last frame is extended to make it divisible, i.e. this process turns a segment with a variable number of frames into a segment with a fixed number of $N$ frames.

\subsection{Deep Learning Models} \label{sec:3-dlmodel}

Each $N$seg will be classified as a chord label through a deep neural net. There are three types of deep neural nets used here:  FCNN, DBN, and RNN.

\subsubsection{Fully-connected Neural Network}

The FCNN is a vanilla neural network with the most basic settings. It is a feedforward neural network, and each layer is fully-connected to the next layer. It applies rectified linear units (ReLUs) as hidden layer activations. It is trained via stochastic gradient descent with back-propagation.

\subsubsection{Deep Belief Network}

The DBN is implemented based on the FCNN. Its multiple hidden layers have sigmoid activations instead of ReLUs.

In the pre-training phase, every pair of adjacent layers (except for the output layer) are trained one pair at a time as restricted Boltzmann machines (RBMs) \cite{hinton2006fast}. In our implementation, the RBM formed by the input layer and the first hidden layer is a Gaussian-Bernoulli RBM, because the input $N$seg feature contains real numbers. The RBMs formed by the hidden layer pairs are Bernoulli-Bernoulli RBMs, because each neuron is stochastic binary \cite{hinton2006reducing}.

In the fine-tuning phase, the network is regarded as a feedforward neural network and trained via stochastic gradient descent with back-propagation.

\subsubsection{Recurrent Neural Network}

The RNN shown in Figure~\ref{fig:3-rnn} is bidirectional with long-short-term-memory (LSTM) hidden units \cite{hochreiter1997long}, or a BLSTM-RNN. LSTM is incorporated in order to relieve gradient vanishing/exploding problem for long sequence training \cite{bengio2009learning}. In our LSTM implementation, all gates employ sigmoid activations, while both the cell and output neuron use hyperbolic tangent activations. For a fixed-length $N$seg input, the RNN is unrolled into $N$ slices, each handling one input frame. A mean pooling operation is added before the output layer to summarize the LSTM outputs.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\columnwidth]{3/figures/rnn.pdf}
\caption{The bidirectional recurrent neural network architecture used in the proposed system. Both hidden layers employ LSTM units in place of normal logistic units. The RNN is expanded to N frames, with mean pooling to summarize results.}
\label{fig:3-rnn}
\end{figure}

\section{Experiments} \label{sec:3-exper}
In this section, we describe a systematic approach to explore and evaluate different system variants of the LVACE framework. We first introduce the datasets, then elaborate the experimental setup, and finally discuss the training and cross-validation (CV) process.

\subsection{Datasets}

For training/cross-validation, we use six datasets of 546 tracks in total. They contain both eastern and western pop/rock songs. They are:
\begin{itemize}
	\item 29 tracks from the JayChou dataset (JayChou29, or J) \footnote{\url{http://www.tangkk.net/label/}\label{f1}};
	\item 20 tracks from a Chinese pop song dataset (CNPop20, or C) \textsuperscript{\ref{f1}};
	\item 26 tracks from the Carole King + Queen dataset (K) dataset \footnote{\url{http://isophonics.net/datasets}\label{f2}};
	\item 191 songs from the USPop dataset (U) \footnote{\url{https://github.com/tmc323/Chord-Annotations}};
	\item 100 tracks from the RWC dataset (R) \footnote{\url{https://staff.aist.go.jp/m.goto/RWC-MDB/}};
	\item 180 tracks from the TheBeatles180 (B) dataset \textsuperscript{\ref{f2}}. 
\end{itemize}
The datasets are notated by their letter codes. For example, the combination of all datasets is denoted as ``CJKURB''.

Both chromagram (-ch) and notegram (-ns) are extracted from each track. Both of them can be transposed to all 12 different keys by circular pitch shifting (for -ch) or pitch shifting with zero paddings (for -ns). For example, a piece of treble chromagram in key of $C$ can be represented as:
\begin{equation}
PCP_C = (C',C\#', D', D\#', E', F', F\#', G', G\#', A', A\#', B'),
\end{equation}
where $X'$ stands for the salience of pitch class $X$. It can be circularly shifted to represent an equivalent PCP in other keys, such as key of $D$:
\begin{equation}
PCP_D = ( D', D\#', E', F', F\#', G', G\#', A', A\#', B', C',C\#').
\end{equation}
As for notegram, although we have pitch saliences instead of pitch class saliences, the same ``pitch shifting'' ideas can still be applied, given that the out-shifted saliences are filled by zeros.

In practice, the original key is considered as a pivot, and the features are circularly shifted to all 12 keys (the amount of transpositions ranging from -6 to 5 semitones). Adjusting the ground truth chord labels accordingly, this results in a 12-time data augmentation, which helps in reducing over-fitting \cite{cho2014improved,humphrey2015exploration}.

\subsection{Experimental Setup}
Under the proposed LVACE framework, possible design choices are:
\begin{itemize}
	\item type of deep neural nets
	\item depth and width of hidden layers (network configurations)
	\item number of frames in segment tiling
	\item input feature representations
	\item amount of training data
\end{itemize}

\begin{table}[h!]
	\caption{Variations considered in this study}
	\centering
	\footnotesize
	\begin{tabular}{|c|c|c|} \hline
		Dimension & Variation \\ \hline
		neural net & FCNN; DBN; BLSTM-RNN \\ \hline
		segment tiling & 1; 2; 3; 6; 9; 12 (seg) \\ \hline
		layer depth & 2; 3; 4 \\ \hline
		layer width & 500; 800; 1000 \\ \hline
		input feature & notegram (-ns); chromagram (-ch) \\ \hline
		amount of training data & JK; JKU; JKUR; JKURB \\ \hline
	\end{tabular}
	\label{tab:3-varexplore}
\end{table}

Our study is based on the settings depicted in Table~\ref{tab:3-varexplore}. For naming conventions: a combination of layer width and depth is denoted as [$width$*$depth$], such as [800*2]; a segmentation tiling scheme is denoted as $N$seg, such as 6seg; a point in this six dimensional hyper-parameter space is denoted by concatenating each parameter with ``-'', such as FCNN-6seg-[800*2]-ch-JKU. The space can be explored by parameter sweeping along a given dimension. Particularly, we will first explore along the {\it layer width} and {\it layer depth}. We then explore the \textit{segment tiling} scheme with fixed {\it layer width} and {\it layer depth}. Following the same strategy, we explore all factors in Table~\ref{tab:3-varexplore}. This process does not search the whole hyper-parameter space. However, it could gain us some insights of the proposed LVACE framework and produce some good system variants as well.

In this context we regard a ``model'' as a crossing point of all dimensions in Table~\ref{tab:3-varexplore}, including training data size. We regard a ``system'' as a full implementation of the LVACE framework, including the feature extraction, segmentation and deep neural nets. However, since all models share the same feature extraction and segmentation processes, we sometimes use the terms ``model'' and ``system'' interchangeably.


\subsection{Training and Cross-validation}\label{sec:3-traintest}

The following training procedures are applied throughout the experiments: 
\begin{itemize}
	\item Each FCNN is trained using mini-batch stochastic gradient descent, and regularized with dropout \cite{srivastava2014dropout} and early-stopping \cite{prechelt1998automatic}. 
	\item Each DBN is pre-trained using contrastive-divergence \cite{hinton2006fast} (CD-10), for 30 epochs with a learning rate of 0.001. It is fine-tuned using the FCNN's training procedure.
	\item Each BLSTM-RNN is trained using an Adadelta optimizer \cite{zeiler2012adadelta}, regularized with dropout and early-stopping. 
	\item All mini-batch stochastic gradient descents use a learning rate of 0.01 and a batch size of 100. 
	\item All early-stopping criteria are monitored using the validation error of the CNPop20 dataset, which is not in any training set. The model with the lowest validation loss will be saved, and if the current validation loss is 0.996 times smaller than the lowest one, the early-stopping patience will increase by the value of the current number of iterations. Training stops when the patience is less than the current number of iterations.
	\item All dropout rates are set to 0.5. 
\end{itemize}
Five-fold cross-validation (CV) is performed throughout all experiments. Each fold is a combination of approximately 1/5 tracks of each dataset. Every model is trained on four folds and cross validated on the remaining fold, resulting in a total number of five training/cross-validation scores, the average of which will be the final score to be reported.

%We provide all implementation details including the training and cross-validation scripts online \footnote{\url{https://github.com/tangkk/tangkk-mirex-ace}}, so that interested readers can repeat the experiments when they have access to the raw audio datasets.

\section{Results and Discussions} \label{sec:3-res}
Throughout this section, we use the MIREX ACE standard evaluation metrics, already introduced in Section~\ref{sec:2-eval}, to report system performances. We report all \textit{WCSR} scores under the \textit{SeventhsBass} evaluation, where a correct classification does {\it not} involve any chord mapping scheme beyond the \textit{SeventhsBass} vocabulary \cite{pauwels2013evaluating}. We use the MusOOEvaluator tool \footnote{\url{https://github.com/jpauwels/MusOOEvaluator}} to generate these scores from all the ground truth and predicted chord sequences. Note that we do not report the segmentation score for every system, because they all share the same GMM-HMM process described in Section~\ref{sec:3-sg}. This process has a segmentation score of 83\% on the JKURB dataset.

In the following discussion, we will analyze the experiment results from a bias-variance perspective \cite{geman1992neural}. Assuming that a model is trained for multiple times over different samples of the same population, with other settings remain unchanged, the model's prediction for a given input will become a random variable. The model's prediction error can thus be expressed as \cite{friedman2001elements}:
\begin{equation}
Prediction\ Error = Bias^2 + Variance + Irreducible\ Error
\end{equation}
Concretely, a model's bias is defined as the expected difference between its prediction and the ground truth. It measures how much a model's predictions are consistently deviating from the true value, and it indicates whether a model contains fundamentally incorrect assumptions. A model's variance, on the other hand, measures the variance (i.e. the statistical variance, which equals the square of the standard deviation) of the model's prediction. It indicates how much a model's predictions will vary across its different realizations with different training samples, or equivalently, how much inconsistencies are there within the predictions. Finally, the irreducible error term could be seen as collection of everything that is not bias or variance, such as the noise or inconsistencies in the data itself.

Bias and variance is highly correlated with over-fitting and under-fitting. A high bias model tends to under-fit the data, while a high variance model tends to over-fit the data. A model that neither over-fits nor under-fits, will have low bias and low variance. The amount of bias-variance can be approximated from a model's training and validation (or cross-validation) score:
\begin{itemize}
	\item A model with high bias (under-fitting) has similar training and validation scores, but none of them is high.
	\item A model with high variance (over-fitting) has a high training score and a low validation score.
\end{itemize}
In other words, a model's bias can be approximated as the value of its training or validation error if the two errors are close to each other, and a model's variance can be approximated as the difference between its training and validation errors. Note that the irreducible error could either appear as bias or variance.

In the following, we will examine how each design choice in Table~\ref{tab:3-varexplore} actually affects the systems' biases and variances. Moreover, we also compare among different types of deep neural nets and a baseline model, and see which one performs the best in the LVACE task.

\subsection{Network configurations} \label{sec:3-p2}

Figure~\ref{fig:3-configs} shows the \textit{WCSR}s of a set of JKU-6seg models with different neural nets, network configurations and input features.
\begin{figure}[h!]
	\centering
	\includegraphics{3/figures/configs.pdf}
	\caption{Exploring the effect of different network configurations. All models are trained with JKU-6seg-ch.}
	\label{fig:3-configs}
\end{figure}
\Hsection{-ch models}
The FCNN has local maximal validation scores when the network has two layers, and it performs worse as  network becomes deeper. The DBN's validation scores are stabilized around 50. In this group of experiments, the training and validation scores are close to each other.

\Hsection{-ns models}
The FCNN's validation scores are all focused around 50. As for the DBN, there is a trend of performance downgrade along the depth dimension. In both cases the differences between training and validation scores are very small.

\Hsection{Remarks}
Firstly, all the FCNN-ch models outperform the FCNN-ns models. This could be largely due to the prior imposed by the chromagram feature. It embeds the knowledge about ``pitch classes'', and it is originally designed for chord recognition tasks. On the other hand, because the notegram is several feature transformations away from the chromagram, it contains no such prior information. This could explain why the FCNN-ch models perform worse as they become deeper. Every extra layer will tend to weaken the prior at the input, and at the same time, these layers try to learn some other regularities that map the chromagrams to the chord labels. The results show that, unfortunately, the deeper networks are unable to learn more useful regularities than the prior knowledge already contained in the chromagram.

Secondly, all the DBN-ns models outperform the DBN-ch models (except for the one at [1000*4]). Note that the only difference between the DBN and the FCNN is the generative pre-training process, which in effect is a strong regularization process that prevents over-fitting. This is sometimes equivalent to increasing the model's bias or decreasing the model's variance. As shown in Figure~\ref{fig:3-configs}, since the variances of the FCNN-ch models are already small, the DBN-ch models perform worse than the FCNN-ch models due to the higher biases. However, since the FCNN-ns models have high variances, the DBN-ns models perform better than the FCNN-ns because of the lower variances.

Thirdly, the performance downgrade of the DBN-ns models starting from [800*3] and [1000*2] could be caused by the well-known gradient vanishing problem in deep networks (with sigmoid activations). This could be verified by monitoring the weight updating process. When the gradient vanishing happens, the average amount of weight updates closer to the input will be much less than those closer to the output, resulting in more errors in the earlier layers, which will be aggregated through the feedforward path to the output.

\subsection{Segment tiling} \label{sec:3-p3}

Figure~\ref{fig:3-nseg} shows the \textit{WCSR}s of a set of JKU-[800*2] models with different neural nets, segment tiling schemes ($N$seg) and input features. Note that [800*2] in BLSTM-RNN means there are a forward and a backward hidden layers, each having 800 LSTM units.
\begin{figure}[h!]
	\centering
	\includegraphics{3/figures/nseg.pdf}
	\caption{Exploring the effect of segment tiling. All models are trained with JKU-ch-[800*2]}
	\label{fig:3-nseg}
\end{figure}

\Hsection{-ch models}
The FCNN tends to perform worse with larger $N$; The DBN tends to perform better with larger $N$; The BLSTM-RNN grows gently when $N$ is less than 3, and remains relatively constant thereafter. They all have very small variances.

\Hsection{-ns models}
Still the FCNN tends to perform worse with larger $N$; Both the DBN and the BLSTM-RNN have the worst performances when $N$ is 1, and they have higher and stable performances when $N$ is greater than 1.
%All models are quite unaffected by $N$seg, particularly when $N$ is greater than 2. Different from those at -ch level, the variances of these models are larger, especially the BLSTM-RNN models.

\Hsection{Remarks}
With a large $N$seg, a model becomes more complex because of a less blurry input, so that one could expect either less bias, or more variance. In the FCNN models we could observe a tendency of slight variance increasing. This tendency has possibly offset the trend of bias decreasing, which we could not see from Figure~\ref{fig:3-nseg}.

For the DBN, as discussed in Section~\ref{sec:3-p2}, the generative pre-training processes could reduce the variances or increase the biases. This is clearly reflected in Figure~\ref{fig:3-nseg} if we compare the DBN's \textit{WCSR}s with the FCNN's. On one hand, this could explain why the DBN-ch models have an increasing trend of performances (as well as a decreasing trend of biases) with a larger $N$, because the DBN-ch model has a much higher bias than the FCNN-ch one when $N$ equals 1. On the other hand, it could also explain why there is a performance boost from $N=1$ to $N=2$ in the DBN-ns models, and that the DBN-ns models have consistently lower variances and higher performances than the FCNN-ns models.

For the BLSTM-RNN models, the training and CV curves are much more spread out than those of the FCNN and the DBN. On one hand, the RNN imposes a weight sharing mechanism across the segment tiling frames. This has an effect of regularization by limiting the number of parameters connecting the input layer to the hidden layer, thus limiting the network's ability to recognize arbitrary dependencies across frames. On the other hand, the RNN also introduces a set of recurrent weights that connect each frame to its next frame. This makes the network more flexible in capturing sequential dependencies between frames. This explains why the training and CV curves are so separated. Particularly, the average training scores of the RNN models are much higher than those of the DBN's and the FCNN's, because the RNN is essentially biased towards problems with sequential natures, and the ACE is one of these problems. Still, we have relatively low CV scores in these RNN models, which lead to high variances. As we will see in the following subsection, this can be remedied by more training data.

\subsection{Amount of training data}\label{sec:3-p5}
Figure~\ref{fig:3-data} shows the \textit{WCSR}s of a set of 6seg-[800*2] models with different neural nets, training data sizes and input features.
\begin{figure}[h!]
	\centering
	\includegraphics{3/figures/data.pdf}
	\caption{Exploring the different training data size. All models are trained with 6seg-ch-[800*2].}
	\label{fig:3-data}
\end{figure}

\Hsection{For both -ch and -ns models}
In all three plots, there are clear trends that increasing the amount of data boosts the models' performances. While the variances of the FCNN and DBN models remain being small, the variances of the BLSTM-RNN models tend to decrease with the increase of data. Interestingly for the FCNN, the increase of data from JKUR to JKURB leads to larger variances and worse CV scores.

\Hsection{Remarks}
Similar to Figure~\ref{fig:3-nseg}, the FCNN's and DBN's training and CV curves as shown in Figure~\ref{fig:3-data} are still very close to each other. It seems that as the amount of data increases, their models have saturated at some point and there is little room for further improvement. On the contrary, the BLSTM-RNN's training and CV curves are much wider apart, and the models tend to generalize better as the data size grows. The results in Figure~\ref{fig:3-data} seem to suggest that we have almost touched a performance ``ceiling'' of the BLSTM-RNN-ch models, but we have yet to reach that of the BLSTM-RNN-ns models.

\subsection{Input feature}\label{sec:3-p6}
From Figure~\ref{fig:3-configs} to~\ref{fig:3-data}, we see that the training and CV curves of the chromagram models are on average much closer than those of the notegram models. This suggests that the prior knowledge contained in the chromagram feature actually introduces bias in the model. On one hand, this may lead to better models if the amount training data is limited (this discussion is not valid for the DBN because of the generative pre-training process). On the other hand, this can also limit the models' improvement when we have sufficient amount of training data. For example in Figure~\ref{fig:3-data}, we see a potential trend that if more data is added to the model, the BLSTM-RNN-ns model will eventually outperform the BLSTM-RNN-ch model, because the BLSTM-RNN-ns has a higher ceiling (the training score) than the BLSTM-RNN-ch.

\subsection{Balanced performance}\label{sec:3-p7}
The above discussions are focused on the overall \textit{WCSR}s of different models. Here we are going to examine the models' performances on specific chords. Note that in our datasets (which we believe are good representatives of pop and rock music in general), the chord distributions are highly skewed (as shown in Table~\ref{tab:3-chorddist}), where the $maj$ and $min$ triads make up almost 70\% of the whole sample population, the $maj7$, $min7$ and $7$ chords constitute more than 20\%, and the portion of other chords are less than 10\%. In the following discussion, we refer to ``common chords'' as the $maj$ and $min$ chords, ``uncommon chords'' as the sevenths chords, including the $maj7$, $min7$ and $7$ chords, and ``long-tail chords'' as all the other chords in the \textit{SeventhsBass} vocabulary. Moreover, we use ``chord'' and ``chord type'' interchangeably to refer to a certain type of chords.

Figure~\ref{fig:3-skewed} shows how different deep neural net models perform on different chords. It is surprising to see that the FCNN and the DBN outperform the BLSTM-RNN only in the \textit{maj} chord category, while the BLSTM-RNN outscores the other two by large margins in most long-tail chords and uncommon chords categories.
\begin{figure}[h!]
	\centering
	\includegraphics[width=\columnwidth]{3/figures/avskewed.pdf}
	\caption{Performance on different chords in different neural nets. All models are trained with 6seg-[800*2].}
	\label{fig:3-skewed}
\end{figure}

Furthermore, we examine the versatilities of different deep neural net models. We measure them using the \textit{ACQA}, which averages the \textit{WCSR}s of all chords with equal weights. Models that over-fit a few chord types tend to give lower \textit{ACQA}s, while those well-balanced ones will have higher \textit{ACQA}s. As shown in Figure~\ref{fig:3-acqa}, the average \textit{ACQA} of the BLSTM-RNN models outscores the average \textit{ACQA}s of the other two types of models by around 10 points.

\begin{landscape}
	\thispagestyle{plain}
	\vspace*{\fill}
	\begin{table*}[h]
		\tiny
		\caption{Distribution of chords in the datasets. (maj and min: 69.9\%; maj7, min7 and 7: 21.3\%; others: 8.8\%)}
		\label{tab:3-chorddist}
		\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\hline
			Dataset & Tracks & maj/5 & maj/3 & maj & maj7/5 & maj7/3 & maj7/7 & maj7 & 7/5 & 7/3 & 7/b7 & 7 & min/5 & min/b3 & min & min7/5 & min7/b3 & min7/b7 & min7\\ \hline
			C & 20 & 3.2 & 4.5 & 38.4 & 0.2 & 0.2 & 0.0 & 10.2 & 0.0 & 0.3 & 1.1 & 9.3 & 2.7 & 0.0 & 20.3 & 0.5 & 0.0 & 0.1 & 9.1\\ \hline
			J & 29 & 4.1 & 8.1 & 32.3 & 1.2 & 0.2 & 0.1 & 6.9 & 0.4 & 1.5 & 2.3 & 5.0 & 0.7 & 1.3 & 15.1 & 0.7 & 0.0 & 0.3 & 19.8\\ \hline
			K & 26 & 4.5 & 3.9 & 52.0 & 0.0 & 0.3 & 0.2 & 5.1 & 0.3 & 0.2 & 0.5 & 6.2 & 0.1 & 0.3 & 14.9 & 0.2 & 0.0 & 0.8 & 10.4\\ \hline
			U & 191 & 2.3 & 3.9 & 54.7 & 0.0 & 0.1 & 0.1 & 3.2 & 0.1 & 0.3 & 0.3 & 8.3 & 0.4 & 0.4 & 15.1 & 0.0 & 0.1 & 0.4 & 10.2\\ \hline
			R & 100 & 2.5 & 4.6 & 42.8 & 0.3 & 0.1 & 0.1 & 8.9 & 0.0 & 0.2 & 0.3 & 7.9 & 0.4 & 0.5 & 15.3 & 0.0 & 0.1 & 0.2 & 15.7\\ \hline
			B & 180 & 2.4 & 1.0 & 66.1 & 0.0 & 0.2 & 0.3 & 0.9 & 0.1 & 0.1 & 0.4 & 8.7 & 0.6 & 0.5 & 15.9 & 0.0 & 0.1 & 0.4 & 2.5\\ \hline
			Weighted Average & & 2.6 & 3.3 & 54.3 & 0.1 & 0.1 & 0.2 & 4.0 & 0.1 & 0.3 & 0.5 & 8.1 & 0.6 & 0.5 & 15.6 & 0.1 & 0.1 & 0.4 & 9.1\\ \hline
		\end{tabular}
	\end{table*}
	\vspace*{\fill}
\end{landscape}

\begin{figure}[h!]
	\centering
	\includegraphics{3/figures/avacqa.pdf}
	\caption{Average ACQAs of \textit{SeventhsBass} Vocabulary. All models are trained with 6seg-[800*2].}
	\label{fig:3-acqa}
\end{figure}

To compare whether the \textit{ACQA} differences among these system variants are significant, we perform a Friedman test \cite{friedman1937use} with Tukey HSD (honest significant difference) \cite{tukey1949comparing} with a $p$-value of 0.05 on the track-wise \textit{ACQA}s. As shown in Figure~\ref{fig:3-friedmanacqa}, both BLSTM-JKURB-ch-6seg-800 and BLSTM-JKURB-ns-6seg-800 are significantly better (no overlap of confidence intervals) than the other systems, and BLSTM-JKURB-ch-6seg-800 is significantly better than BLSTM-JKURB-ns-6seg-800 as well. This concludes that the BLSTM-RNN models are significantly better than the FCNN and the DBN models in terms of \textit{ACQA}s, or balanced performances.
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\columnwidth]{3/figures/ftestacqa.eps}
	\caption{Friedman test with Tukey HSD: ACQAs of different system variants}
	\label{fig:3-friedmanacqa}
\end{figure}

Now we have concrete evidence that the BLSTM-RNN is a better neural network in solving the LVACE problem than the other two models. It is reasonable to think that the BLSTM-RNN regards its input as a {\it sequence of frames}, while fully-connected networks (in this context the FCNN and DBN) regard their inputs as \textit{flat vectors}. Therefore, while the BLSTM-RNN tries to look for regularities within \textit{each pair of consecutive frames} along the time direction, the FCNN or DBN would search for regularities within \textit{every point of the flat vector} as if they are not time related at all. In some cases, the latter approach is more efficient given that the input feature has already encoded certain prior information about music (e.g. chromagram contains the information about pitch classes). Nevertheless, it overlooks the ``sequential order of frames'', which probably causes the over-fitting of root position chords and the under-fitting of chord inversions.

\subsection{Baseline comparison} \label{sec:3-p9}
Finally, we compare our LVACE framework with the Chordino. It should be emphasized that Chordino is the only suitable baseline because: (1) Our framework resembles Chordino in terms of the segmentation and feature extraction processes; (2) Chordino is the only other system that supports seventh chords and chord inversions.
\begin{figure}[h!]
	\centering
	\includegraphics{3/figures/cpwcsr.pdf}
	\caption{Performance comparison between system representatives and Chordino on WCSRs. All models are trained with JKURB-6seg-ns-[800*2].}
	\label{fig:3-compchordino}
\end{figure}

We choose one representative for each type of deep neural net, all trained and cross-validated with JKURB-6seg-ns-[800*2], and compare them with the Chordino using the standard MIREX ACE categories. As shown in Figure~\ref{fig:3-compchordino}, the representative of BLSTM-RNN outperforms the Chordino by large margin in \textit{Sevenths} and \textit{SeventhsBass}, and it scores fairly close to the Chordino in \textit{MajMin} and \textit{MajMinBass}. The other two representatives are not performing as good as the Chordino in most categories.

We perform a Friedman test with Tukey HSD ($p=0.05$, using the track-wise results) to test whether the differences in the \textit{SeventhsBass} \textit{WCSR}s are significant. As shown in Figure~\ref{fig:3-friedmanwcsrbl}, BLSTM-JKURB-ns-6seg-[800*2] is significantly better than the other systems as well as the Chordino.
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\columnwidth]{3/figures/ftestwcsrbl.eps}
	\caption{Friedman test with Tukey HSD: WCSRs compared with the baseline}
	\label{fig:3-friedmanwcsrbl}
\end{figure}

In terms of \textit{ACQA}, as shown in Figure~\ref{fig:3-acqachordino}, Chordino outperforms both the FCNN's and DBN's representatives, but the most balanced system is the BLSTM-RNN's representative. We again perform a Friedman test with Tukey HSD ($p=0.05$, using the track-wise results) to test whether the differences in \textit{ACQA}s are significant. As shown in Figure~\ref{fig:3-friedmanacqabl}, the BLSTM-JKURB-ns-6seg-[800*2] system is again significantly better than the other systems as well as the Chordino.
\begin{figure}[h!]
	\centering
	\includegraphics{3/figures/cpacqa.pdf}
	\caption{Performance comparison between system representatives and Chordino on ACQAs. All models are trained with JKURB-6seg-ns-[800*2].}
	\label{fig:3-acqachordino}
\end{figure}
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\columnwidth]{3/figures/ftestacqabl.eps}
	\caption{Friedman test with Tukey HSD: ACQAs compared with the baseline}
	\label{fig:3-friedmanacqabl}
\end{figure}

\section{MIREX 2016 Results}
MIREX ACE 2016 features 4 sets of systems implemented by four different groups:
\begin{itemize}
\item CM1: Chordino, from Queen Mary University of London;
\item DK*: features three variants of the LVACE approach proposed in this chapter \cite{deng2016mirex} (note that DK4, which is from another design approach, is skipped in the following discussion);
\item FK*: implemented by Korzeniowski and Widmer, featuring the systems in two of their papers \cite{Korzeniowski2016feature,Korzeniowski2016convolutional}
\item KO1: shineChords, a time-frequency reassign approach proposed by Khadkevich and Omologo \cite{khadkevich2011time}.
\end{itemize}

\subsection{Dataset and Vocabulary}
It should be noted that except for CM3, whose model parameters are all manually specified, all other systems involve extensive training.

KO1's description does not mention its training set, but it is probably trained with the Isophonic dataset (MIREX 2009 set) \cite{burgoyne2014comparative}. This can be inferred by noticing that, compared with other systems, it has too high a \textit{SeventhsBass} score in Isophonic, while not as high in other test sets.

FK* are trained with the Isophonic, Robbie Williams \footnote{\url{https://www.researchgate.net/publication/260399240\_Chord\_and\_Harmony\_annotations\_of\_the\_first\_five\_albums\_by\_Robbie\_Williams}}, RWC and the public part of McGill Billboard \footnote{\url{http://ddmal.music.mcgill.ca/billboard}} dataset.

DK* are trained with the USPop and RWC dataset, which are not part of MIREX ACE 2016's test sets. Note that DK1 is the DBN-6seg-ch-[800,800]-UR system and DK2 is the BLSTM-RNN-6seg-ch-[800,800]-UR system.

In terms of chord vocabulary, both DK3, FK* and KO1 only support \textit{majmin}; CM1 supports a large vocabulary including most types in \textit{SeventhsBass}; and DK1\&2 support exactly the \textit{SeventhsBass}.
\thispagestyle{plain}
\begin{table*}[htbp]
\centering
\scriptsize
\caption{MIREX 2016 Results}
\label{tab:3-mirex2016}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}\hline
Algorithm & R & Mm & MmB & S & SB & Seg & UnderSeg & OverSeg \\ \hline
Isophonics 2009\\ \hline
CM1 & 78.56 & 75.41 & 72.48 & 54.67 & 52.26 & 85.90 & 87.17 & 86.09\\ \hline
DK1 & 79.21 & 76.19 & 74.00 & 66.02 & 64.15 & 85.71 & 82.62 & 91.23\\ \hline
DK2 & 77.84 & 74.49 & 71.93 & 61.61 & 59.47 & 85.82 & 82.72 & 91.28\\ \hline
DK3 & 80.03 & 77.55 & 74.79 & 68.40 & 65.88 & 85.81 & 82.50 & 91.53\\ \hline
DK4 & 76.05 & 72.96 & 71.41 & 62.77 & 61.44 & 78.19 & 87.97 & 72.43\\ \hline
FK2 & 86.09 & 85.53 & 82.24 & 74.42 & 71.54 & 87.76 & 85.79 & 90.73\\ \hline
FK4 & 82.28 & 80.93 & 78.03 & 70.91 & 68.26 & 85.62 & 82.40 & 90.89\\ \hline
KO1 & 82.93 & 82.19 & 79.61 & 76.04 & 73.43 & 87.69 & 85.66 & 91.24\\ \hline
Billboard 2012 \\ \hline
CM1 & 74.15 & 72.22 & 70.21 & 55.35 & 53.40 & 83.64 & 85.31 & 83.39\\ \hline
DK1 & 75.28 & 73.57 & 71.87 & 59.98 & 58.53 & 83.35 & 80.26 & 88.52\\ \hline
DK2 & 73.77 & 71.69 & 69.86 & 58.66 & 57.00 & 83.57 & 80.40 & 88.70\\ \hline
DK3 & 75.92 & 74.75 & 72.69 & 53.42 & 51.67 & 83.39 & 79.97 & 88.92\\ \hline
DK4 & 72.59 & 70.85 & 69.78 & 56.29 & 55.36 & 76.13 & 87.72 & 70.05\\ \hline
FK2 & 85.64 & 85.38 & 82.55 & 60.70 & 58.38 & 87.62 & 86.09 & 90.13\\ \hline
FK4 & 79.23 & 78.62 & 76.20 & 56.53 & 54.51 & 85.09 & 81.98 & 89.94\\ \hline
KO1 & 77.45 & 75.58 & 73.51 & 57.68 & 55.82 & 84.16 & 82.80 & 87.44\\ \hline
Billboard 2013 \\ \hline
CM1 & 71.16 & 67.28 & 65.20 & 48.99 & 47.17 & 81.54 & 83.11 & 82.63\\ \hline
DK1 & 72.06 & 68.69 & 67.26 & 54.54 & 53.29 & 80.82 & 77.58 & 88.06\\ \hline
DK2 & 70.18 & 66.54 & 64.66 & 52.97 & 51.41 & 80.85 & 77.68 & 88.02\\ \hline
DK3 & 72.39 & 68.53 & 66.55 & 48.99 & 47.28 & 80.76 & 77.26 & 88.30\\ \hline
DK4 & 69.56 & 65.83 & 64.78 & 51.81 & 50.93 & 74.55 & 86.31 & 69.18\\ \hline
FK2 & 80.07 & 77.89 & 75.42 & 55.41 & 53.22 & 82.94 & 82.43 & 86.80\\ \hline
FK4 & 74.66 & 71.85 & 69.44 & 51.93 & 49.80 & 80.61 & 77.19 & 88.70\\ \hline
KO1 & 75.36 & 71.39 & 69.43 & 53.57 & 51.78 & 81.63 & 79.61 & 87.75\\ \hline
JayChou 2015 \\ \hline
CM1 & 72.75 & 72.08 & 65.48 & 54.39 & 48.98 & 86.60 & 86.89 & 86.91\\ \hline
DK1 & 74.70 & 73.87 & 70.33 & 54.98 & 52.25 & 86.76 & 82.78 & 91.79\\ \hline
DK2 & 72.19 & 72.55 & 69.10 & 54.09 & 51.46 & 87.09 & 83.35 & 91.75\\ \hline
DK3 & 75.01 & 74.75 & 63.56 & 49.27 & 40.24 & 86.76 & 82.54 & 92.08\\ \hline
DK4 & 71.51 & 69.03 & 65.93 & 50.07 & 47.45 & 78.11 & 87.87 & 70.56\\ \hline
FK2 & 79.51 & 78.66 & 68.15 & 50.69 & 42.34 & 86.81 & 85.43 & 88.56\\ \hline
FK4 & 76.13 & 75.44 & 64.36 & 49.69 & 40.74 & 84.55 & 81.22 & 88.95\\ \hline
KO1 & 78.73 & 77.69 & 66.87 & 54.16 & 44.55 & 88.46 & 87.12 & 90.11\\ \hline
RobbieWilliams 2016 \\ \hline
CM1 & 81.90 & 78.25 & 76.05 & 57.92 & 55.90 & 87.96 & 88.96 & 87.45\\ \hline
DK1 & 81.50 & 77.77 & 76.10 & 68.88 & 67.34 & 87.03 & 83.22 & 92.11\\ \hline
DK2 & 79.01 & 75.97 & 73.57 & 65.26 & 62.98 & 87.20 & 83.40 & 92.23\\ \hline
DK3 & 81.85 & 78.56 & 76.16 & 74.71 & 72.55 & 86.98 & 82.95 & 92.34\\ \hline
DK4 & 78.92 & 75.15 & 73.66 & 66.72 & 65.34 & 81.82 & 88.44 & 76.88\\ \hline
FK2 & 88.53 & 87.23 & 84.19 & 82.57 & 79.88 & 90.04 & 88.62 & 91.88\\ \hline
FK4 & 83.37 & 80.96 & 78.42 & 77.04 & 74.76 & 87.22 & 84.50 & 91.02\\ \hline
KO1 & 83.55 & 80.33 & 78.16 & 73.54 & 71.39 & 88.04 & 85.39 & 91.68\\ \hline
\end{tabular}
\end{table*}


\subsection{Results and Discussions}
Table~\ref{tab:3-mirex2016} shows the MIREX ACE 2016 results (for brevity, in the tables, B = \textit{Bass}, Mm = \textit{MajMin}, MmB = \textit{MajMinBass}, R = \textit{Root}, S = \textit{Sevenths}, SB = \textit{SeventhsBass}, Seg = \textit{Segmentation Quality}). Within the large vocabulary context of this thesis, the main focus is the \textit{SeventhsBass} column.

In the Isophonic test, KO1 gets the highest score. This is probably because of its over-fitting of the data, as explained previously. Note that it scores much lower in tests that it has not previously seen, such as JayChou 2015 (The same as the JayChou29). Both FK2 and FK4 come next to KO1, but unfortunately they are all informed with this test set during training. Since it is not clear to what extend the systems over-learn the data, the results of this test is not statistically valid to deduce any reasonable conclusions.

In Billboard 2012 and 2013 tests, the best performances are both from DK1. Although FK* learn from part of the test set, they somehow does not outperform DK1. Moreover, DK1 outscores KO1 by about 3 points in both tests.

JayChou 2015 test is probably the most convincing one, since none of the systems are learning this set. Moreover, more than 20\% of it are chords other than \textit{maj}, \textit{min} and \textit{sevenths}. This portion is much more than those in the other test sets. The performance on this test could well demonstrate a system's vocabulary versatility. This can be noticed by making comparisons among the DK* systems, where DK1 and DK2 perform much better than DK3 in \textit{SeventhsBass}. The best two systems in this test are DK1 and DK2, outscoring the FK* systems by around 10 points, and the KO1 system by around 8 points.

Robbie Williams test somehow shows the reverse ranking of JayChou 2015 test. DK3 leads both DK1 and DK2 by about 5 and 10 points. FK* are the best performing ones, but they might as well over-fit the data. It is interesting to note that KO1 scores 71.39, slightly less than DK3's 72.55. This strongly indicates that this test set is mainly composed of \textit{maj}, \textit{min} and \textit{sevenths} (probably > 90\%), which is very similar to the chord composition of the Isophonic set. Therefore systems that only support a small chord vocabulary will have a big advantage by not being able to confuse \textit{maj} and \textit{min} chords with the long-tail chords.

\section{On Smaller Vocabularies}
This chapter is mainly developed for large vocabulary, specifically, the \textit{SeventhsBass} vocabulary. In order to do a thorough comparison between the proposed system framework and other approaches, a set of experiments are also conducted using two smaller vocabularies, i.e., 1, \textit{MajMin} (the same as \textit{majmin}); 2, \textit{Full121} vocabulary (already introduced in Section~\ref{sec:2-largevocab}).

For \textit{MajMin} implementation, each label in the training set will be mapped to its \textit{MajMin} form using a normal chord mapping scheme \cite{harte2010towards,pauwels2013evaluating}. As a result, each deep neural net will be trained on a dataset with only \textit{MajMin} labels. For \textit{Full121} implementation, we use Mauch's chord mapping strategy \cite{mauch2010automatic}. In the following all systems are implemented to support exactly the \textit{MajMin} or the \textit{Full121}.

\subsection{On MajMin}
Table~\ref{tab:3-overallres} shows the results, where all systems, except for Chordino, only support the \textit{MajMin} vocabulary. On one hand, the overall performances of these systems are only fairly comparable with those in Section~\ref{sec:3-p9}, on the other hand, the \textit{ACQA} of these systems are all much lower because of a much smaller vocabulary than Chordino's.
\begin{table}[h!]
\scriptsize
\centering
\caption{Overall WCSR scores; All systems, besides Chordino, are trained with CJKUR-[800*2], tested on the TheBeatles180 dataset, with only MajMin vocabulary support.}
\label{tab:3-overallres}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}\hline
System & B & Mm & MmB & R & S & SB & Seg & ACQA \\ \hline
Chordino & 76.41 & 74.30 & 71.40 & 77.19 & 52.99 & 50.60 & 83.87 & 16.61\\ \hline
FCNN-ch & 74.71 & 73.25 & 71.22 & 75.74 & 65.08 & 63.18 & 83.57 & 8.42\\ \hline
DBN-ch & 77.04 & 75.50 & 73.39 & 78.10 & 67.26 & 65.30 & 83.78 & 8.71\\ \hline
BLSTM-RNN-ch & 76.88 & 75.05 & 72.84 & 78.11 & 66.58 & 64.50 & 83.74 & 8.75\\ \hline
FCNN-ns & 72.65 & 70.00 & 68.14 & 73.35 & 62.72 & 60.98 & 83.59 & 7.81\\ \hline
DBN-ns & 72.91 & 70.96 & 69.14 & 73.66 & 63.33 & 61.66 & 83.44 & 8.05\\ \hline
BLSTM-RNN-ns & 76.04 & 73.98 & 71.99 & 76.85 & 65.85 & 64.03 & 83.76 & 8.54\\ \hline
\end{tabular}
\end{table}

Table~\ref{tab:3-detailres} shows the \textit{SeventhsBass} categorical breakdown. Take DBN-ch as a representative, since it only supports \textit{MajMin}, its $maj$ and $min$ scores are much higher than the Chordino's. As the two chord types take up the majority of the dataset's population, DBN-ch has a much higher \textit{SeventhsBass} score in Table~\ref{tab:3-overallres}. However, by relaxing the evaluation strength from \textit{SeventhsBass} to \textit{Sevenths} and to \textit{MajMin}, the performance difference between DBN-ch and Chordino becomes much smaller.
\begin{landscape}
\thispagestyle{plain}
\vspace*{\fill}
\begin{table*}[h!]
\tiny
\caption{Detail SeventhsBass WCSR scores. All systems, besides Chordino, are trained with CJKUR-[800*2], with only MajMin vocabulary support. M = major, m = minor, N = N.C (no chord). The \%B row shows the composition of chords in the test set.}
\label{tab:3-detailres}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\hline
\%B & 2.01 & 0.95 & 63.31 & 0.02 & 0.17 & 0.27 & 0.82 & 0.08 & 0.06 & 0.39 & 8.33 & 0.61 & 0.44 & 14.99 & 0.01 & 0.06 & 0.41 & 2.37 & 4.63\\ \hline
 & M/5 & M/3 & M & M7/5 & M7/3 & M7/7 & M7 & 7/5 & 7/3 & 7/b7 & 7 & m/5 & m/b3 & m & m7/5 & m7/b3 & m7/b7 & m7 & N\\ \hline
Chordino & 19.9 & 17.1 & 54.4 & 0.0 & 0.0 & 0.0 & 55.6 & 0.0 & 0.0 & 5.7 & 41.0 & 0.0 & 0.0 & 54.3 & 0.0 & 0.0 & 0.0 & 51.0 & 2.2\\ \hline
FCNN-ch & 0.0 & 0.0 & 77.6 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 74.0 & 0.0 & 0.0 & 0.0 & 0.0 & 2.8\\ \hline
DBN-ch & 0.0 & 0.0 & 80.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 76.8 & 0.0 & 0.0 & 0.0 & 0.0 & 3.0\\ \hline
BLSTM-RNN-ch & 0.0 & 0.0 & 79.3 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 78.2 & 0.0 & 0.0 & 0.0 & 0.0 & 2.5\\ \hline
FCNN-ns & 0.0 & 0.0 & 76.4 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 64.2 & 0.0 & 0.0 & 0.0 & 0.0 & 2.9\\ \hline
DBN-ns & 0.0 & 0.0 & 76.3 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 68.6 & 0.0 & 0.0 & 0.0 & 0.0 & 2.9\\ \hline
BLSTM-RNN-ns & 0.0 & 0.0 & 79.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 74.7 & 0.0 & 0.0 & 0.0 & 0.0 & 2.7\\ \hline
\end{tabular}
\end{table*}
\vspace*{\fill}
\end{landscape}

\subsection{On Full121}
Table~\ref{tab:3-full} shows the \textit{WCSR}s of \textit{Full121} vocabulary with the same TheBeatles180 test set. Table~\ref{tab:3-fullhp} and ~\ref{tab:3-fullmbk} show the \textit{Full121} scores by other systems using a slightly larger 216-track Isophonic test set, which is extended from the TheBeatles180 test set. Table~\ref{tab:3-fullhp} is reported in a 3-fold cross-validation manner, while Table~\ref{tab:3-fullmbk} is in 5-fold.
\begin{table}[h]
\footnotesize
\centering
\caption{Full121 scores. Tested on the TheBeatles180 dataset. All systems are trained with CJKUR-[800*2]-ch, with Full121 vocabulary.}
\label{tab:3-full}
\begin{tabular}{|c|c|c|}\hline
System & WCSR & Seg \\ \hline
FCNN & 73.43 & 83.77 \\ \hline
DBN & 75.46 & 83.92 \\ \hline
BLSTM-RNN & 73.63 & 83.80 \\ \hline
\end{tabular}
\end{table}

The computational processes of both \textit{CP} and \textit{RCO} metrics are essentially similar to that of the \textit{WCSR}, and they only accept ``exact chord match'' under the vocabulary \cite{ni2012end,mauch2010automatic}.
\begin{table}[h]
\footnotesize
\centering
\caption{Full121 scores. Tested on the Isophonic 2009 dataset with 3-fold cross-validation\cite{ni2012end}}
\label{tab:3-fullhp}
\begin{tabular}{|c|c|c|}\hline
System & Chord Precision (CP) & Seg \\ \hline
CH (Chordino) & 50.31 & N/A \\ \hline
HP (Harmonic Analyzer) & 70.26 & N/A\\ \hline
\end{tabular}
\end{table}

It is difficult to make any conclusive statement out of these three different tables, which are from slightly different test sets, and reported in different ways. But it is reasonably fair to say that the proposed system framework is not bad at all in handling the \textit{Full121} vocabulary.

\begin{table}[h]
\footnotesize
\centering
\caption{Full121 scores. Tested on the Isophonic 2009 dataset with 5-fold cross-validation \cite{mauch2010automatic}}
\label{tab:3-fullmbk}
\begin{tabular}{|c|c|c|}\hline
System & Relative Correct Overlap (RCO) & Seg \\ \hline
trained MBK & 41.8 & N/A \\ \hline
MBK & 56.7 & 77.9 \\ \hline
MBK-NNLS & 61.8 & N/A \\ \hline
\end{tabular}
\end{table}


\section{Summary} \label{sec:3-concln}
This chapter presents an in-depth discussion of a hybrid GMM-HMM and deep neural nets LVACE approach. The study is motivated by a current research gap in ACE, i.e. the general negligence of segmentation-classification separation approach and large vocabulary system. This serves as the basis for the proposed chord inversions supportable segmentation-classification separated LVACE system framework. The system framework leverages handcrafted feature extraction and segmentation processes, and plugs in various deep neural nets to classify the chord within each segment. Experimental results indicate that:
\begin{itemize}
	\item The chromagram feature contains prior knowledge about musical pitch class that increases the bias and limits the potential improvement of the models.
	\item The BLSTM-RNN can learn regularities from the notegram feature that potentially outperforms the chromagram feature.
	\item The BLSTM-RNN's representative system (with all available training data) has significantly better \textit{WCSR} and \textit{ACQA} than the FCNN's one, the DBN's one, and the Chordino.
\end{itemize}

Despite the best system variant significantly outperforms the baseline system, all training and CV scores presented in this chapter are still far less than 100\%. This indicates either there is large bias in the LVACE framework itself, or there is irreducible error in the underlying data. We speculate three potential causes as explained in the following.

Firstly, the performance of the proposed framework is upper-bounded by the segmentation performance of the GMM-HMM process introduced in Section~\ref{sec:3-sg}, and the performance of this process on the JKURB set is 83\%.

Secondly, the segment tiling process introduces bias to the system, since it assumes a chord can be correctly recognized after we tile its original features into several frames of averaged features. This process could help prevent over-fitting by regularizing the degree of freedom of the input, but at times it scarifies important information conveyed in the original variable-length features.

The above two points set a hard performance limit of the proposed LVACE framework: unless the chord segmentation technique is perfect and the segment tiling process is completely excluded, one could not expect a system with very low bias.

Thirdly, there is non-negligible amount of noise in the ground truth annotations themselves. Inevitably, due to differences in musical training, human annotators sometimes disagree, particularly on long-tail chords \cite{humphreyfour}. This results in a glass ceiling for LVACE: unless there are more data for uncommon and long-tail chords and they are more consistently labeled, all efforts for improving LVACE will be hindered by the lack of skewed class training and data consistency.

In a very strict sense, there is not any ``gold standard'' if human annotators themselves might disagree with each other. But in a loose sense, there could be a ``gold standard'' if:
\begin{itemize}
	\item all annotations are done by only one annotator, or
	\item all annotations are done by multiple annotators (much more than two).
\end{itemize}
In the former case, the only annotator ``dictates'' a local ``gold standard'', so that whenever a machine tries to learn from the data, it actually targets at this annotator's ``style''. In the latter case, multiple annotators decide a ``gold standard'' in a way such as majority vote or data fusion \cite{koopsintegration,klein2004sensor}, so that a trained model actually aims at the optimal ``style'' that minimizes the objections among these annotators. Therefore, although the ``gold standard'' is indeed an important issue, we still have to design a system that ``learns well''.

We believe that the next step of LVACE research should focus more on improving the recognition accuracies on uncommon and long-tail chords. That is, instead of considering the overall \textit{WCSR} of a large vocabulary, attention should also be given to the balanced metric, such as \textit{ACQA}. Although we have pointed out that the BLSTM-RNN is very promising in handling large vocabulary with inversions, we have yet to explored possible ways to train the network under such ``imbalanced class population '' scenario \cite{chawla2004editorial}. More importantly, we should spend greater efforts on data collection, particularly of long-tail chords, and at the same time ensure the data integrity and consistency, in the future development of LVACE.

Finally, as a potential application, a piece of timed chord sequence output can be combined with the \textit{.lrc} file \footnote{\url{https://en.wikipedia.org/wiki/LRC\_(file\_format)}} of the same track to become a chord-lyrics sheet, which is one of the most frequently used pop music sheet formats. Figure~\ref{fig:3-aihenjiandan} shows an example output of this automatic process \footnote{\url{https://github.com/tangkk/songtranspose}}.
\begin{figure}[h]
    \centering
        \includegraphics[trim={0 6cm 0 0},clip,width=1.2\columnwidth]{3/figures/aihenjiandan.pdf}
    \caption{Chord-lyrics output of \textit{Aihenjiandan} (by Taiwan singer-songwriter David Tao)}
    \label{fig:3-aihenjiandan}
\end{figure}
Note that in this case the lyrics might not be perfectly aligned with chords, since an \textit{.lrc} file only has information on the start and end time of each line, instead of each word. A more sophisticated solution would be to use the audio-lyrics alignment tool \cite{mauch2010lyrics} as a preprocessing stage before the chord-lyrics matching.


% ---------------------------------------------------------------------------
%: ----------------------- end of thesis sub-document ------------------------
% ---------------------------------------------------------------------------

