% this file is called up by thesis.tex
% content in this file will be fed into the main document

%: ----------------------- name of chapter  -------------------------
\chapter{An End-to-end Recurrent Neural Network Approach}\label{cp:endtoend} % top level followed by section, subsection

%: ----------------------- paths to graphics ------------------------

% change according to folder and file names
\ifpdf
    \graphicspath{{X/figures/PNG/}{X/figures/PDF/}{X/figures/}}
\else
    \graphicspath{{X/figures/EPS/}{X/figures/}}
\fi

As recapped in Section~\ref{sec:2-rnnpm} that by now there are basically three deep learning approaches for ACE:
\begin{itemize}
\item local classification - global smoothing \cite{humphrey2012rethinking};
\item local feature learning - global classification - global smoothing \cite{boulanger2013audio,sigtia2015audio};
\item local feature learning - local classification - global smoothing \cite{zhou2015chord}.
\end{itemize}
All of these approaches incorporate a ``global smoothing'' stage after classification. This is a step that requires prior domain knowledge which does not belong to the deep learning approach itself. This chapter is going to study on an ``end-to-end'' approach that tries to employ a new neural network training scheme to on one hand achieve a more balanced performance on both ordinary and long-tail chords, and on the other hand eliminate the need of ``global smoothing'' altogether. The system presented here belongs to the ``local feature extraction - global classification'' category.


%: ----------------------- contents from here ------------------------
\section{System Overview}\label{sec:4-sysover}
Figure \ref{fig:4-sysover} shows the system overview, which mainly contains a feature extraction module and a BLSTM-RNN sequence decoding module. The former is the same as the bass-treble chromagram (-ch level) extraction process presented at Chapter~\ref{cp:ghmm}.

\begin{figure}[htb]
\centering
\includegraphics[width=0.4\columnwidth]{4/figures/sys-chordino-like.pdf}
\caption{BLSTM-RNN end-to-end ACE System Overview. The raw audio is transformed by a chordino-like feature extraction frontend into a piece of chromagram, and then decoded by a BLSTM-RNN into chord sequence.}
\label{fig:4-sysover}
\end{figure}

\section{Implementation}\label{sec:4-blstm}
The key module in this system is the BLSTM-RNN. This section will first briefly discuss the concerns of RNN variable-length sequence training, and then elaborate on the implementation of the BLSTM-RNN.

\subsection{RNN}
A RNN can model the conditional probability of a label sequence $l$ given its corresponding input feature sequence $x$. Define the label alphabet as $L$, and let both $x$ and $l$ have the same length $T$ in the unit of time. The conditional probability modeled by the RNN can be written as:

\begin{equation}\label{eq:4-rnnprob}
p(l|x) = \prod_{t=1}^T y_{l_t}^t  \quad\quad l\in L \quad and \quad |l| = |x|
\end{equation}

where $l_t$ is the $t^{th}$ element of $l$ and $y_{l_t}^t$ is the probability of predicting label $l_t$ at time $t $given $x$. At time $t$, $y_{l_t}^t$ can also be written as $p(l_t|x)$, which is the value of the $l^{th}$ slot of RNN prediction at time $t$. This RNN can be trained using a maximum likelihood approach by updating the network weights towards a direction that increases the conditional probability of ground truth sequences given their input representation sequences. Thus the cost function will simply be equation \ref{eq:4-rnnprob} (or the logarithm of it) given both $l$ and $x$ are in the training set.

Note that unlike ASR, training an ACE RNN using a connectionist temporal classification (CTC) cost function \cite{graves2006connectionist} is problematic and theoretically may not yield better results. This is because ACE emphasizes segmentation quality, but ASR does not. The necessity of CTC's forward and backward algorithm stems from the fact that there could be many paths that lead to the same label sequence, and this in turn is based on the fact that the length of a path could be longer than that of the label sequence. However, using CTC for transduction between two sequences of the same length will inevitably reduce the CTC cost function back to equation \ref{eq:4-rnnprob}. But this does not rule out other possible novel applications of CTC cost function to ACE, especially in combination of sequence force alignment techniques \cite{sheh2003chord,mauch2010lyrics}.

\subsection{Implementation of BLSTM-RNN}
Figure \ref{fig:4-blstm} shows the graphical model of the BLSTM-RNN, which has a forward and a backward hidden layer both with 800 LSTM units. The output layer is a \#-chord-way softmax layer, corresponding to the posterior probabilities of all classes in SeventhsBass. The input layer has 24 nodes with continuous values, corresponding to an input chroma.

\begin{figure}[htb]
\centering
\includegraphics[width=0.6\columnwidth]{4/figures/blstm.pdf}
\caption{The BLSTM-RNN. Both the forward and backward hidden layers contain 800 LSTM units}
\label{fig:4-blstm}
\end{figure}

\subsection{Data Augmentation}
The training/validation datasets used in this system are the same as those in Chapter~\ref{cp:ghmm}. To generate training data, firstly all raw audios are transformed to bass-treble chromagram representations. The original segment-wise ground truth annotations are upsampled to become frame-wise annotations that have 1-to-1 mappings to their input representations. Due to the absence of phase information in chromagram, all data can be circularly transposed to all 12 keys in order to have 12 times data augmentation.

\subsection{Training Schemes}
A random split of 80\% of them are used as training set, and the other 20\% split as validation set. During each iteration, there two different training schemes are applied:
\begin{itemize}
\item completely random: one random training case is fed into an Adadelta optimizer \cite{zeiler2012adadelta} to update the network connections. Each training case contains 500 frames (subject to the end-of-track boundary condition) of audio content with ground truth labels.
\item even chance: this is a more delegated training method for imbalanced dataset \cite{chawla2004editorial}, which will be described below
\end{itemize}
Considering the skewed distribution of chord classes in the training datasets \cite{burgoyne2011expert}, a randomly sampling scheme will inevitably draw cases based on that same distribution, which probably causes lack of exposure to long-tail chords. Therefore, scheme 2 is invented to let each class has an equal chance to be seen during the training process. Concretely, the scheme is formalized as follows:
\begin{algorithm}[h]
	\caption{Even Chance Training for RNN}
	\label{alg:4-ectrain}
	\begin{algorithmic}
	\REQUIRE
	training data set - $(X,y)$;
	number of chord classes - $ydim$
	\STATE $od$ = balancedOrderedDict($y$, $ydim$)
	\STATE $iteration$ = $0$
	\WHILE {stopping criteria == \FALSE}
	\IF {mod($iteration$, $ydim$) is $0$}
	\STATE $classorderidx$ = random\_shuffle($0$:$ydim$-$1$)
	\ENDIF
	\STATE $bcidx$ = batch indexes drawn from $od$ using $classorderidx$
	\STATE update network using Adadelta optimizer with $(X,y)$ indexed by $bcidx$
	\STATE $iteration$++
	\ENDWHILE
	\end{algorithmic}
\end{algorithm}
The core of this procedure is the computation of $od$, which is a dictionary of (track index, chord change position) indexed by chord classes:
\begin{algorithm}[h]
	\caption{balancedOrderedDict}
	\label{alg:4-bod}
	\begin{algorithmic}
	\REQUIRE
	labels of training data set - $y$;
	number of chord classes - $ydim$
	\FOR {each class $i$ from $0$ to $ydim-1$}
	\STATE initialize an empty $od[i]$
	\ENDFOR
	\FOR {each track index $j$ in $y$}
	\FOR {each frame poistion $k$ in $y[j]$} 
	\IF {$k$ is a chord change position}
	\STATE append ($j$,$k$) to $od[y[j][k]]$
	\ENDIF
	\ENDFOR
	\ENDFOR
	\RETURN
		$od$
	\end{algorithmic}
\end{algorithm}
Each entry of $od$ contain a list of (track index, chord change position). When drawn from the list, a random item is returned. Therefore, the $bcidx$ in Algorithm~\ref{alg:4-ectrain} contains for each class a random case in a random track starting right from the chord change position. Each case also lasts for 500 frames (subject to the end-of-track boundary condition). This scheme guarantees a even chance of training for each class, give that each has at least one example in the training set.

The training is regularized by dropout with 0.5 probability, and further regularized by early stopping when there is no improvement after 10 validations, with 1000 iterations per validation. The BLSTM-RNN is implemented and trained under the framework of Theano. The model with the best validation score will be saved for testing. For reproducible research, the implementation of the whole system is made available online\footnote{\url{https://github.com/tangkk/tangkk-mirex-ace/}}.

\section{Results and Discussion}\label{sec:4-eval}
This section presents evaluation results of several proposed system variants in terms of the current MIREX. The test set is still the benchmarking TheBeatles180. First of all there will be a intra-comparison among variants with different configurations (Table~\ref{tab:4-varexplore}), then a baseline comparison with Chordino will be discussed.

\begin{table}
\caption{Variations considered in this study.}
\centering
\footnotesize
\begin{tabular}{|c|c|c|} \hline
Dimension & Configurations \\ \hline
training scheme & completely random (CR); even chance (EC) \\ \hline
training data size & J; CJ; CJK; CJKU; CJKUR \\ \hline
\end{tabular}
\label{tab:4-varexplore}
\end{table}

\begin{table}[htb]
\caption{Overall WCSR scores. B = Bass, Mm = MajMin, MmB = MajMinBass, R = Root, S = Sevenths, SB = SeventhsBass, Seg = Segmentation Quality; The asterisks indicate systems with even chance training.}
\label{tab:4-overallres}
\centering
\footnotesize
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}\hline
System & B & Mm & MmB & R & S & SB & Seg & SOSB \\ \hline
chordino & 76.41 & \textbf{74.30} & \textbf{71.40} & \textbf{77.19} & 52.99 & 50.60 & \textbf{83.87} & 299.01\\ \hline
J& 67.66 & 65.38 & 61.80 & 69.06 & 54.21 & 50.86 & 71.72 & 230.30 \\ \hline
J* & 68.48 & 65.05 & 61.90 & 68.96 & 53.94 & 51.10 & 71.85 & 298.42 \\ \hline
CJ & 72.59 & 68.93 & 65.49 & 72.91 & 58.33 & 55.24 & 74.20 & 288.73 \\ \hline
CJ* & 71.29 & 66.43 & 62.77 & 71.65 & 52.13 & 48.97 & 70.57 & \textbf{320.10} \\ \hline
CJK & 74.25 & 70.63 & 69.00 & 73.88 & 59.53 & 58.04 & 77.63 & 244.78 \\ \hline
CJK* & 72.05 & 67.59 & 64.70 & 72.24 & 55.29 & 53.05 & 72.21& \textbf{301.62} \\ \hline
CJKU & 74.48 & 71.04 & 69.52 & 74.16 & \textbf{61.00} & \textbf{59.65} & 76.91 & 213.67 \\ \hline
CJKU* & 72.10 & 68.93 & 67.27 & 71.68 & 57.10 & 55.63 & 74.48 & 248.65 \\ \hline
CJKUR & \textbf{76.46} & 70.33 & 68.71 & 75.98 & 60.35 & \textbf{58.92} & 76.88 & 262.48 \\ \hline
CJKUR* & 74.24 & 70.74 & 68.44 & 74.32 & 58.94 & 56.88 & 73.34 & \textbf{327.81} \\ \hline
\end{tabular}
\end{table}

\subsection{Overall Performance}
Table~\ref{tab:4-overallres} shows the overall results in SeventhsBass evaluation. Generally speaking, the two best variants (in terms of SB score), CJKU and CJKUR, largely outperform chordino in SB and S, and are fairly compared with chordino in Mm and MmB, though they significantly fall short in terms of segmentation quality. In the SB column, from J to CJKUR the scores increase in a diminishing way as the amount of training data increase. Such increase is understandable since more training data leads to better generalization ability in an unknown test set drawn from a similar population. But the diminishing return is undesirable, especially for the small improvement from CJK to CJKU, and the no improvement from CJKU to CJKUR.

As for the SOSB column, it is very obvious that systems with EC training have much better performances on long-tail chords. While their SB scores are all lower than the random case training counterpart, their SOSB scores are all much higher.

%With a detail look into the per chord type results comparison among CJK, CJKU and CJKUR in Table~\ref{tab:4-detailres}, it will again be understandable. There is a small deficiency (1\%) of CJKU than CJK in terms of recognizing the dominating major triads (M), which overshadows huge improvement in minor triads (m) and dominant sevenths (7). A further discussion of the confusions and inconsistency issue may shed more light on the diminishing return.
\begin{landscape}
\thispagestyle{plain}
\begin{table*}[h]
\scriptsize
\caption{Detail SeventhsBass WCSR scores. M = Major, m = minor, N = no chord. The \%B row shows the composition of chords in the test dataset. The asterisks indicate systems with even chance training.}
\label{tab:4-detailres}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\hline
\%B & 2.01 & 0.95 & 63.31 & 0.02 & 0.17 & 0.27 & 0.82 & 0.08 & 0.06 & 0.39 & 8.33 & 0.61 & 0.44 & 14.99 & 0.01 & 0.06 & 0.41 & 2.37 & 4.63\\ \hline
 & M/5 & M/3 & M & M7/5 & M7/3 & M7/7 & M7 & 7/5 & 7/3 & 7/b7 & 7 & m/5 & m/b3 & m & m7/5 & m7/b3 & m7/b7 & m7 & N\\ \hline
chordino & 19.9 & 17.1 & 54.4 & 0.0 & 0.0 & 0.0 & 55.6 & 0.0 & 0.0 & 5.7 & 41.0 & 0.0 & 0.0 & 54.3 & 0.0 & 0.0 & 0.0 & 51.0 & 2.2\\ \hline
J & 11.9 & 31.7 & 62.0 & 0.0 & 0.0 & 0.0 & 22.3 & 0.0 & 0.9 & 16.9 & 2.8 & 0.9 & 0.1 & 42.2 & 0.0 & 0.0 & 0.0 & 38.6 & 3.2\\ \hline
J* & 17.5 & 30.6 & 61.1 & 7.8 & 0.0 & 0.0 & 31.8 & 0.0 & 8.7 & 39.1 & 7.6 & 2.5 & 3.6 & 40.5 & 0.0 & 0.0 & 2.7 & 44.8 & 3.4 \\ \hline
CJ & 18.9 & 34.4 & 65.8 & 0.0 & 0.0 & 0.0 & 37.6 & 0.0 & 0.6 & 31.2 & 3.6 & 0.3 & 0.0 & 52.8 & 0.0 & 0.0 & 0.0 & 43.5 & 3.0\\ \hline
CJ* & 17.2 & 24.0 & 55.3 & 0.0 & 0.0 & 1.1 & 45.2 & 0.0 & 33.0 & 44.5 & 9.8 & 3.6 & 10.5 & 56.5 & 0.0 & 0.0 & 0.0 & 19.5& 2.8\\ \hline
CJK & 9.8 & 24.4 & 72.5 & 0.0 & 0.0 & 0.0 & 25.0 & 0.0 & 0.0 & 20.4 & 2.5 & 0.0 & 0.0 & 42.5 & 0.0 & 0.0 & 0.0 & 47.7 & 3.5\\ \hline
CJK* & 14.8 & 25.1 & 61.0 & 13.1 & 0.0 & 0.4 & 41.3 & 0.0 & 0.0 & 34.0 & 14.0 & 2.1 & 4.4 & 53.8 & 0.0 & 0.0 & 5.5 & 32.0 & 3.2\\ \hline
CJKU & 9.8 & 20.5 & 71.5 & 0.0 & 0.0 & 0.0 & 9.4 & 0.0 & 0.0 & 0.0 & 21.5 & 0.1 & 0.0 & 56.0 & 0.0 & 0.0 & 0.0 & 24.9 & 3.0\\ \hline
CJKU* & 12.5 & 22.6 & 69.1 & 0.0 & 0.0 & 1.8 & 10.5 & 0.0 & 2.9 & 18.7 & 15.8 & 4.9 & 1.5 & 43.6 & 0.0 & 0.0 & 4.7 & 40.1 & 2.3\\ \hline
CJKUR & 9.6 & 25.3 & 69.8 & 0.0 & 0.0 & 0.0 & 18.1 & 0.0 & 2.3 & 18.8 & 19.4 & 0.0 & 1.7 & 59.3 & 0.0 & 0.0 & 0.0 & 38.1 & 2.5 \\ \hline
CJKUR* & 15.1 & 29.0 & 65.6 & 0.0 & 0.0 & 3.9 & 41.0 & 0.0 & 7.9 & 25.9 & 23.4 & 4.1 & 5.9 & 57.2 & 0.0 & 0.0 & 18.5 & 30.4 & 2.8\\ \hline
\end{tabular}
\end{table*}
\end{landscape}

\subsection{Confusions and Inconsistencies}
Here is a quick recap of the SeventhsBass evaluation scores: SB score is the lowest one and reflects the true SeventhsBass score; the difference between S and SB reflects the amount of confusions (bass confusion) between root positions and inversions; the difference between MmB and SB reflects the amount of confusions (sevenths confusion) between triads ($maj$, $min$ and their inversions) and tetrads (sevenths and their inversions); and the difference between Mm and SB reflects the sum of all these confusions.

From J to CJKUR, the amount of bass confusion decrease (from 4.6\% to 1.3\%), while the amount of sevenths confusion remains at a certain level (around 10\%). Specifically, this implies whenever an inversion gets misclassified, it is easier to be misclassified as a chord in a different root.

As the amount of data increase, due to a random sampling training scheme, the inversions are getting marginalized by the dominating root position chords. The small classification hyperplanes that originally taken by inversions are encroached by the ever enlarging hyperplanes of the dominating classes. This is truly reflected in Table~\ref{tab:4-detailres}, where the original good classification rates of M/5, M/3 and 7/b7 by CJ (both C and J alone contain around 20\% portion of chord inversions), are actually destroyed by the union of K and U, though this trend is reversed a bit by adding the R set.

However, there shouldn't have been such drastic encroachment given that the affected classes all have distinguishable unique definitions and all their instances are consistently labelled. Therefore unfortunately the author believe C,J,K and U are not consistently labelled, which is very plausible. Actually, both C and J are annotated by the author of this thesis, K and B (TheBeatles180) are annotated in C4DM\footnote{\url{http://www.isophonics.net/}}, U is annotated in NYU\footnote{\url{https://github.com/tmc323/Chord-Annotations}} and R by RWC Group\footnote{\url{https://staff.aist.go.jp/m.goto/RWC-MDB/}}. As Humphrey et al. \cite{humphreyfour} point out, multiple annotators may have annotation inconsistency. This is demonstrated originally with two annotators labeling the same dataset. But this phenomenon logically also holds for multiple annotators labeling different datasets from the same population. It is estimated this is the reason behind the downgrading of $M/5$, $M/3$ and $7/b7$. Specifically, it is human annotators' disagreement on whether or not to label chord inversions, as in some scenarios, a human annotator tends to label a chord as root position regardless of the non-salient bass note (e.g., inconsistency of $C$ v.s. $C/E$), or in some other scenarios, to put a chord in root position on the bass regardless of the chord structure (e.g., inconsistency of $C/E$ v.s. $Em$ (this is a wrong annotation) or $Em(b6)$, notably in the dynamic context of $F-C/E-Dm$ or $Dm-C/E-F$).

This also explains the up and down irregularities in the WCSR trend of $M7$ and $m7$. While in this case there is only one source of ground truth inconsistency: whether or not to label a seventh extension, which is not related to the root. That is possibly the reason why the amount of confusions between MmB and SB remains the same level.

\subsection{About Dominant 7th}
Special attention is given to the $7$ chord in Table~\ref{tab:4-detailres}, where chordino scores as high as 41\%, CJKU scores 21.5\%, CJKUR* 23.4\% and all others score less than 20\%. There is a huge gap between the baseline and the proposed systems' performances. Chordino is originally manually tuned against TheBeatles180 dataset, and that's why it gets such a high score. This test set contain various song styles, including a lot of bluesy songs with progressions of dominant 7th. And many of these are rendered in a way with dynamic bass line rather than static bass line. But the population of $7$s in CJK, or at least in CJ, is mostly static, so that the system does not learn much about dynamic $7$s. By adding dataset U and R, which contain more examples of dynamic $7$s, the system learns them and makes better prediction of dynamic $7$s in the test set.

\subsection{Segmentation Quality}
All variants underperform chordino in segmentation quality in a significant amount. Though it improves with increasing size of training data. Chordino segments chords by applying a very high self-transition weight in its HMM's state transition matrix. The BLSTM-RNN does not have this prior knowledge. It learns segmentation by looking at segmentation examples in the training data. Unlike chord labels, segmentation information is implicit. Every training case contains 500 frames, which is about 23 seconds under our feature extraction settings. This corresponds to approximately 10-30 chord changes. And the BLSTM-RNN model has to learn segmentation and classification all at once. An above 77\% score is very promising indeed, but it still need to be improved.

\subsection{The Effect of Even Chance Training}
As demonstrated in Table~\ref{tab:4-overallres}, generally the incorporation of EC training downgrade the overall performances in all SeventhsBass categories, including the segmentation quality. However, it improves a lot the versatility metric SOSB. Without using EC, none of the systems can surpass Chordino in SOSB, but with EC, two of them outscore Chordino for as many as 20 points. This can be reflected in Table~\ref{tab:4-detailres}, where systems score higher in long-tail chords such as $M/5$, $M/3$ and $7/b7$ mostly with EC than without EC. And those non-zero scores for the VERY long-tail chords, such as $m7/5$, $m7/b7$ and $m/b3$ are mostly found in EC trained entries. The major drawback of EC is that it tends to downgrade the scores of ordinary categories such as $M$ and $m$.

\section{Summary}\label{sec:4-concln}
This chapter proposes an end-to-end ``local feature extraction - global classification'' BLSTM-RNN based system that supports SeventhsBass vocabulary. This system has a chordino-like feature extraction frontend, and can be compared with chordino in a statistically fair way. To cater for large vocabulary recognition, an even chance training scheme is designed. Several variants of the system are trained and implemented. Evaluation results show some variants largely outperform chordino in terms of large vocabulary scores, and they are almost as good as chordino in terms of small vocabulary scores, although having much worse segmentation qualities. The evaluation results demonstrate a well structured and trained BLSTM-RNN is better than a dedicated handcrafted Gaussian-HMM in terms of sequence modeling capability. But this model still has its deficiency in terms of sequence segmentation.

Through detail analyzing the results, a plausible issue of annotation inconsistency is identified. As suggested in Chapter~\ref{cp:background}, a workaround would be to always use annotations provided by one single source/annotator, or, to the other extreme, always use multiple annotators and apply majority vote or data fusion. %But this is not practical. A better solution is to embrace subjectivity and try to incorporate it into objective evaluations. But all in all, the ultimate evaluation tool remains a fully subjective evaluation, that user experience matters the most.

Even chance training has been demonstrated very efficient in improving recognition accuracies on long-tail chords. It also boosts some VERY long-tail chord from zero score to non-zero. However, it sacrifices the performances of large population chords, and thus downgrades the overall performances in all but the versatility categories.

To improve this system, the key thing to be done is to improve RNN's segmentation quality, since it determines an upperbound of system performance, and the scores of all other metrics scale in proportion to segmentation quality. This can possibly be done through a deeper RNN model, or combination of a segmentation-RNN with a labeling-RNN in a hierarchical way.




% ---------------------------------------------------------------------------
%: ----------------------- end of thesis sub-document ------------------------
% ---------------------------------------------------------------------------

